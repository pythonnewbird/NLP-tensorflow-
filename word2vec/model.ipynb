{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences=['该 词向量 数据 包含 很多 现有 公开 的 词向量 数据 所 欠缺 的 短语',\n",
    "           '墨玉 河 和田 河 玉龙喀什 河 白玉 河 喀什 河 叶尔羌 河 克里雅 河 玛纳斯 河',\n",
    "           '腾讯 此次 公开 的 中文 词向量 数据 包含 中文 词汇 其中 每个 词 对应 一个 向量'\n",
    "           '该 词向量 数据 包含 很多 现有 公开 的 词向量 数据 所 欠缺 的 短语'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences,vocab_size):\n",
    "    count=[['UNK',-1]]\n",
    "    word_list = ' '.join(sentences).split(' ')\n",
    "    count.extend(collections.Counter(word_list).most_common(vocab_size-1))\n",
    "    vocab=dict()\n",
    "    for word,_ in count:\n",
    "        vocab[word]=len(vocab)\n",
    "    data=list()\n",
    "    unk_count=0\n",
    "    #假设整个语料都是连贯的\n",
    "    for word in word_list:\n",
    "        if word in vocab:\n",
    "            index=vocab[word]\n",
    "        else:\n",
    "            index=0\n",
    "            unk_count+=1\n",
    "        data.append(index)\n",
    "    count[0][1]=unk_count\n",
    "    index2word=dict(zip(vocab.values(),vocab.keys()))\n",
    "    return data,count,vocab,index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size,num_skips,window):\n",
    "    global data_index\n",
    "    assert batch_size%num_skips==0\n",
    "    assert num_skips<=2*window\n",
    "    batch=np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    span=2*window+1\n",
    "    buffer=collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index=(data_index+1)%len(data)\n",
    "    for i in range(batch_size//num_skips):\n",
    "        context=window\n",
    "        targets2avoid=[window]\n",
    "        for j in range(num_skips):\n",
    "            while context in targets2avoid:\n",
    "                context=random.randint(0,span-1)\n",
    "            targets2avoid.append(context)\n",
    "            batch[i*num_skips+j]=buffer[window]\n",
    "            labels[i*num_skips+j,0]=buffer[context]\n",
    "        #因为最大长度为span，最后插入一个的同时头部也会弹出一个\n",
    "        buffer.append(data[data_index])\n",
    "        data_index=(data_index+1)%len(data)\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data, count, vocab, index2word = build_vocab(sentences,vocab_size=20)\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 包含 -> 2 词向量\n",
      "5 包含 -> 13 该\n",
      "5 包含 -> 3 数据\n",
      "5 包含 -> 7 很多\n",
      "5 包含 -> 8 现有\n",
      "5 包含 -> 6 公开\n",
      "7 很多 -> 5 包含\n",
      "7 很多 -> 8 现有\n",
      "7 很多 -> 4 的\n",
      "7 很多 -> 2 词向量\n",
      "7 很多 -> 3 数据\n",
      "7 很多 -> 6 公开\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "batch, labels = generate_batch(batch_size=12, num_skips=6, window=3)\n",
    "for i in range(12):\n",
    "    print(batch[i], index2word[batch[i]],\n",
    "      '->', labels[i, 0], index2word[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size=20\n",
    "unigrams = [ c / vocabulary_size for token, c in count ]\n",
    "batch_size = 12\n",
    "embedding_size = 10  \n",
    "window = 2      \n",
    "num_skips = 4  \n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ana\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-11-908cb7b40001>:41: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs=tf.placeholder(tf.int32,shape=[batch_size])\n",
    "    train_labels=tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "    valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "    input_ids=train_inputs\n",
    "    labels=tf.reshape(train_labels,[batch_size])\n",
    "    # [vocabulary_size, emb_dim] - input vectors\n",
    "    input_vectors = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),\n",
    "        name=\"input_vectors\")\n",
    "    # [vocabulary_size, emb_dim] - output vectors\n",
    "    output_vectors = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),\n",
    "        name=\"output_vectors\")\n",
    "    label_matrix=tf.reshape(tf.cast(labels,dtype=tf.int64),[batch_size,1])\n",
    "    #负采样,其中labels_matrix为正确的输出词，采样的时候会跳过这些词，num_sampled为采样个数，distortion即为公式(3-4)中的幂指数\n",
    "    sampled_ids,_,_=(tf.nn.fixed_unigram_candidate_sampler(\n",
    "        true_classes=label_matrix,\n",
    "        num_true=1,\n",
    "        num_sampled=6,\n",
    "        unique=True,\n",
    "        range_max=vocabulary_size,\n",
    "        distortion=0.75,\n",
    "        unigrams=unigrams))\n",
    "    center_vects=tf.nn.embedding_lookup(input_vectors,input_ids)\n",
    "    context_vects=tf.nn.embedding_lookup(output_vectors,labels)\n",
    "    sampled_vects=tf.nn.embedding_lookup(output_vectors,sampled_ids)\n",
    "    incorpus_logits=tf.reduce_sum(tf.multiply(center_vects,context_vects),1)\n",
    "    incorpus_probabilities=tf.nn.sigmoid(incorpus_logits)\n",
    "    sampled_logits=tf.matmul(center_vects,sampled_vects,transpose_b=True)\n",
    "    outcorpus_probabilities=tf.nn.sigmoid(-sampled_logits)\n",
    "    #有点疑问\n",
    "    outcorpus_loss_persample=tf.reduce_sum(tf.log(outcorpus_probabilities),1)\n",
    "    loss_persample=-tf.log(incorpus_probabilities)-outcorpus_loss_persample\n",
    "    loss=tf.reduce_sum(loss_persample)/batch_size\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(.4).minimize(loss)\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(input_vectors + output_vectors), 1, keep_dims=True))\n",
    "    normalized_embeddings = (input_vectors + output_vectors) / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    average_loss=0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, window)\n",
    "        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30235264, -0.04517799,  0.59678119,  0.40015423, -0.06895108,\n",
       "        -0.22758423, -0.27130318, -0.28929746, -0.37225044,  0.1945584 ],\n",
       "       [-0.17173713,  0.64888752,  0.25322616,  0.24661313,  0.30968028,\n",
       "        -0.26631936,  0.32117972, -0.00290067, -0.01225195,  0.39289582],\n",
       "       [ 0.27336422, -0.70834923, -0.15362176, -0.20753181, -0.20081806,\n",
       "        -0.25580165, -0.42221516, -0.10078827, -0.08484125, -0.23549929],\n",
       "       [ 0.08434121, -0.55544859, -0.15186371, -0.26071402, -0.50537533,\n",
       "         0.10399426, -0.08743931, -0.2068812 ,  0.36351934, -0.38015571],\n",
       "       [ 0.31436932, -0.35464245, -0.49916646, -0.57809579, -0.14422184,\n",
       "        -0.02473691, -0.16658098,  0.21907637, -0.243789  , -0.18828212],\n",
       "       [ 0.19494556, -0.39189124,  0.11740357,  0.10454261, -0.49063367,\n",
       "         0.53153014, -0.19069931, -0.31554052,  0.264925  , -0.23311336],\n",
       "       [ 0.24521826, -0.20055458,  0.20772356, -0.10521682, -0.37379035,\n",
       "         0.49444723, -0.34406686,  0.02753356, -0.52306467, -0.26170984],\n",
       "       [ 0.0631272 , -0.14405988, -0.0028419 , -0.03695912, -0.40309021,\n",
       "         0.76745248, -0.17156895,  0.02934333,  0.08908185, -0.42917362],\n",
       "       [ 0.15540692, -0.03828518, -0.00439965, -0.19309105, -0.34147507,\n",
       "         0.79999655, -0.24402887,  0.24427606, -0.17210336, -0.17787448],\n",
       "       [ 0.04674333, -0.48213896, -0.4210898 , -0.60964036,  0.00870891,\n",
       "        -0.05703503, -0.00537795,  0.08266129,  0.38781908, -0.23618843],\n",
       "       [-0.11210696, -0.43325844, -0.48482233, -0.5015673 ,  0.11737403,\n",
       "        -0.11235996,  0.28222394,  0.13725966,  0.23887263, -0.36213905],\n",
       "       [-0.02094882,  0.01488918, -0.60641164, -0.54418463,  0.2018491 ,\n",
       "        -0.52427208,  0.09677614,  0.07517724, -0.01618797,  0.06770486],\n",
       "       [ 0.46247968, -0.58709997,  0.1995492 , -0.03404717, -0.45960987,\n",
       "        -0.06179513, -0.34243187, -0.21184482, -0.14887924, -0.03293206],\n",
       "       [ 0.04825667, -0.26744643, -0.67970973, -0.38187379, -0.19353007,\n",
       "        -0.44839877, -0.07816613,  0.12490655, -0.03308711, -0.23872378],\n",
       "       [-0.1778668 ,  0.65495896, -0.46737188, -0.25571901,  0.19746166,\n",
       "        -0.09332249,  0.29527515,  0.10660259,  0.21741441,  0.24908376],\n",
       "       [-0.31607699,  0.50363511,  0.10977559,  0.10780107,  0.15632978,\n",
       "         0.02430148,  0.44972312,  0.1738365 ,  0.18039267,  0.57683128],\n",
       "       [-0.37576917,  0.51321363,  0.13295965,  0.09411035,  0.18138078,\n",
       "        -0.03371997,  0.5686385 ,  0.01423245,  0.11379854,  0.44534916],\n",
       "       [-0.3146188 ,  0.80478108,  0.05208223,  0.02762258,  0.25902289,\n",
       "        -0.156985  ,  0.28172246, -0.0228673 ,  0.10199599,  0.26045367],\n",
       "       [-0.29012787,  0.70234108,  0.06123095, -0.0080355 ,  0.49323183,\n",
       "        -0.15799032,  0.13028127, -0.22761375,  0.14025223,  0.2490752 ],\n",
       "       [-0.00888398,  0.63880569,  0.40074462,  0.12866892,  0.32769531,\n",
       "        -0.13458483,  0.17894942, -0.3427985 , -0.04745138,  0.37069374]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
